# -*- coding: utf-8 -*-
"""PartAq2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-cUD5c5qvBVLR1zK9VgIcvaD_pJbRpmF
"""

!pip install tensorflow-gpu
!nvidia-smi

import numpy as np
import tensorflow as tf
from tensorflow.keras import models, optimizers, activations
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten, Activation, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt

#to test on small dataset
# from google.colab import drive
# drive.mount('/content/drive')
# import os
# os.chdir('drive/My Drive/dl_cs6910/Assignment2')
# train_data_dir = 'drive/My Drive/dl_cs6910/Assignment2/inaturalist_12K/train/'
# test_data_dir = 'drive/My Drive/dl_cs6910/Assignment2/inaturalist_12K/val/'
# !ls
#from give dataset link
# %%capture
!curl -SL https://storage.googleapis.com/wandb_datasets/nature_12K.zip > nature_12K.zip
!unzip nature_12K.zip

# https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/load_data/images.ipynb#scrollTo=crs7ZjEp60Ot
# import pathlib
# import PIL
# import PIL.Image
# !ls {"inaturalist_12K/train/Fungi"}
# train_data_dir = pathlib.Path("inaturalist_12K/train/")
# test_data_dir = pathlib.Path("inaturalist_12K/test/")
# data_dir = pathlib.Path(data_dir)
# image_count = len(list(data_dir.glob('*/*.jpg')))
# print(image_count)
# fungi = list(data_dir.glob('Fungi/*'))
# PIL.Image.open(str(fungi[0]))
# train_data_dir = pathlib.Path("inaturalist_12K/")
# test_data_dir = pathlib.Path("inaturalist_12K/")



class CNN(object):
    def __init__(self):
        return
    def train(self, input_shape, pool_size, m, filter_org, k, activation_function,batch_normalization, dropout, data_aug, neurons_in_dense_layer):
        no_of_filters = [m]*5
        filters_k = (k, k)
        #input layer
        model = models.Sequential()
        model.add(Conv2D(no_of_filters[0], filters_k, input_shape = input_shape))
        model.add(Activation(activation_function))
        
        if batch_normalization:
            model.add(BatchNormalization())
            
        model.add(MaxPooling2D(pool_size))

        #convolutional layers
        _layers = range(1, len(no_of_filters))
        for _layer in _layers:
     
            model.add(Dropout(dropout))
            model.add(Conv2D(filter_org * _layer * no_of_filters[_layer], filters_k))
            model.add(Activation(activation_function))
            if batch_normalization:
                model.add(BatchNormalization())
            model.add(MaxPooling2D(pool_size))

        #dense layer
        model.add(Flatten())
        model.add(Dense(neurons_in_dense_layer))
        model.add(Activation(activation_function))
        
        if batch_normalization:
            model.add(BatchNormalization())
        model.add(Dropout(dropout))
        
        #output layer
        model.add(Dense(10))
        model.add(Activation('softmax'))
        model.summary()
        
        return model



!pip install wandb
import wandb
from wandb.keras import WandbCallback

def generate_data(directory, data_desc, data_aug, batch_size, image_shape):
    if(data_desc == "test"):
        data = ImageDataGenerator(rescale=1./255)
        data = data.flow_from_directory(
            directory,
            target_size=image_shape,
            color_mode="rgb",
            batch_size=batch_size,
            class_mode="categorical",
            shuffle=True,
            seed=42
        )
        return data
    else:
        if data_aug:
            data = ImageDataGenerator(featurewise_center = True,
                                      brightness_range=None,
                                      width_shift_range=0.2,
                                      height_shift_range=0.2, 
                                      shear_range=0.2,
                                      zoom_range=0.2,
                                      horizontal_flip=True,
                                      rotation_range=30,
                                      fill_mode='reflect',
                                      rescale=1./255,
                                      validation_split=0.1
            )
        else:
            data = ImageDataGenerator(rescale=1./255, validation_split=0.1)
            
        train_data = data.flow_from_directory(
            directory,
            target_size=image_shape,
            color_mode="rgb",
            batch_size=batch_size,
            class_mode="categorical",
            shuffle=True,
            seed=42,
            subset="training"
        )
        
        val_data = data.flow_from_directory(
            directory,
            target_size=image_shape,
            color_mode="rgb",
            batch_size=batch_size,
            class_mode="categorical",
            shuffle=True,
            seed=42,
            subset="validation"
        )
        return train_data, val_data

def train():
    
    m = 16 #no. of filter in each layer
    k = 3 #layer size
    params = dict(
#       no_of_filters = [m]*5, #no. of filters
        m = m,
        filter_org = 2,
        k = k, #kernel size
        activation_function = "relu",
        batch_normalization = True,
        dropout = .2,
        data_aug = False,
        neurons_in_dense_layer = 32
    )
    batch_size = 64
    pool_size = (2,2)
    image_shape = (227,227)
    input_shape=(227, 227,3)
    
    wandb.init(config=params, project='test_assignment2_test_run', entity='Shree kanti', settings=wandb.Settings(start_method="fork"))
    config = wandb.config
    wandb.run.name='m: '+str(config.m)+'k'+str(config.k)+'batch_size'+str(batch_size)+'dropout'+str(config.dropout)
    
    #preparing dataset 
    train_data_dir = 'inaturalist_12K/train/'
    test_data_dir = 'inaturalist_12K/val/'
    train, val = generate_data(train_data_dir, "train", params['data_aug'], batch_size, image_shape)
    test = generate_data(test_data_dir, "test", params['data_aug'], batch_size, image_shape)

    #model creation and evaluation
    cnn_model = CNN()
    model = cnn_model.train(input_shape, pool_size, config.m, config.filter_org, config.k,
                    config.activation_function, config.batch_normalization, config.dropout,
                    config.data_aug, config.neurons_in_dense_layer)
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    h = model.fit(train, validation_data=val, epochs=10, callbacks=[WandbCallback()])
    matrices= {'accuracy':h.history['accuracy'], 'val_accuracy':h.history['val_accuracy'],}
    wandb.log(matrices)
    plt.plot(h.history['accuracy'], label='train_accuracy')
    plt.plot(h.history['val_accuracy'], label = 'val_accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy on test/val data')
    plt.ylim([0, 1])
    plt.legend(loc='lower right')
    model.summary()
    model.save('final_best_model_'+str(config.m)+str(config.filter_org)+str(config.k)+str(config.dropout))



if __name__ == "__main__":
  m = [16, 32] #no. of filter in each layer
  k = [3] #kernel length to build shape of kernel
  filter_org = [2, 1]
  activation_function = ["relu"]
  batch_normalization = [True, False]
  dropout = [.2, .3, .4, .5]
  data_aug = [True, False]
  neurons_in_dense_layer = [32]
  sweep_config = {
      "name": "assign2_b",
      "description": "finding best hyperparameter",
      "metric": "Val Accuracy",
      "method": "grid",
      "project": "assignment2",
      "parameters": {
          "m": {
              "values": m
          },
          "filter_org": {
              "values": filter_org
          },
          "k": {
              "values": k
          },
          "activation_function": {
              "values": activation_function
          },
          "batch_normalization": {
              "values": batch_normalization
          },
          "dropout": {
              "values": dropout
          },
          "data_aug": {
              "values": data_aug
          },
          "neurons_in_dense_layer": {
              "values": neurons_in_dense_layer
          }
      }
  }
  # creating the sweep
  sweep_id = wandb.sweep(sweep_config, project="test_assignment2_test_run")
  wandb.agent(sweep_id, function=train)

