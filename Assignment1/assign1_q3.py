# -*- coding: utf-8 -*-
"""Assign1_Q3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cu0vjGkq6zaMfddOqd-QpVMDWCYX1nwX
"""

from keras.datasets import fashion_mnist
import numpy as np
import matplotlib.pyplot as plt

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
train_images = np.array(train_images)
train_images = train_images / 255.0
test_images = np.array(test_images)
test_images = test_images / 255.0
print(train_images.shape)
print(test_images.shape)

class NN(object):
  def __init__(self, num_inputs, hidden_layers, num_outputs):
    self.num_inputs = num_inputs
    self.hidden_layers = hidden_layers
    self.num_outputs = num_outputs

    # nuerons in layers
    layers = [num_inputs] + hidden_layers + [num_outputs]

    # randomized weights and biases
    self.weights = []
    self.bias = []
    for i in range(len(layers)-1):
      self.weights.insert(i, (np.random.rand(layers[i+1], layers[i]) - 0.5))
      self.bias.insert(i, (np.random.rand(layers[i+1],1) - 0.5))
    
    
  def sigmoid(self, x):
    # print("----------------sigmoid _____________________")
    # print(x[0])
    x = x.T
    y = np.zeros(x.shape)
    # print("\n____________________________{}".format(y.shape))
    for i in range(y.shape[0]):
      # print(np.exp(-x[i]))
      y[i] = 1.0 / (1 + np.exp(-x[i]))
    # print("---------------------------------------------------first {}\n\n".format(y[0][0]))
    # print(y[0])
    # y = 1 / (1 + np.exp(-x))
    # print("--------------------------------------------result of sigmoid")
    # print(y.T)
    return y.T

  def d_sigmoid(self, x):  
    y = self.sigmoid(x)
    y = y * (1 - y)
    return y

  def softmax(self, x):
    # print("in softmax")
    # print(x.shape)
    # print(x)
    x = x.T
    y = np.zeros(x.shape)
    # print(y.shape)
    for i in range(x.shape[0]):
      y[i] = np.exp(x[i])/sum(np.exp(x[i]))
    # print("\n{}".format(y.shape))
    # print(y[0])
    # A = np.exp(Z) / sum(np.exp(Z))
    return y.T

  def softmax_num_sable(self, x):
    # print("in stable softmax")
    # print(x.shape)
    # print(x)
    x = x.T
    y = np.zeros(x.shape)
    # print(y.shape)
    for i in range(x.shape[0]):
      exps = x[i] - np.max(x[i])
      exps = np.exp(exps)
      y[i] = exps/sum(exps)
    return y.T

  def forward_prop(self, X):
    hiden_operation = 1;
    self.ai = {}
    self.hi = {}
    self.hi[hiden_operation - 1] = X
    self.ai[hiden_operation - 1] = X

    for w,b in zip(self.weights, self.bias):
      if(hiden_operation < len(self.weights)):
        # print("-kkkkkkkkkkkkkkkkkk----------------------------------------------------------------------------------------------------b[0] {} b: \n{}\n\n".format(b[0], b.T))
        # print("-----------------------------------------------------------------w.h\n\n")
        self.ai[hiden_operation] = w.dot(self.hi[hiden_operation-1]) + b
        # print(w.dot(self.hi[hiden_operation-1]))
        # print("---------------------------------------------------------------a hidden\n\n")
        # print(w.shape)
        # print(self.hi[hiden_operation-1].shape)
        # print(self.ai[hiden_operation].shape)
        # print(self.ai[hiden_operation][0])
        self.hi[hiden_operation] = self.sigmoid(self.ai[hiden_operation])
        hiden_operation += 1
        
      else:
        self.ai[hiden_operation] = w.dot(self.hi[hiden_operation-1]) + b
        # print("_________next hidden")
        # print(self.ai[hiden_operation].shape)
        self.hi[hiden_operation] = self.softmax_num_sable(self.ai[hiden_operation])
    # print("y_hat---------------------------------------------------------------")
    # print(self.hi[hiden_operation])
    # print("\n\n{}".format(self.hi[hiden_operation]))
    return self.hi, self.ai, self.hi[hiden_operation]

  def one_hot(self, y):
    one_hot_Y = np.zeros((len(y), max(y) + 1))
    one_hot_Y[np.arange(len(y)), y] = 1
    one_hot_Y = one_hot_Y
    return one_hot_Y

  def backward_prop(self, h, a, y_hat, y, loss_function):
    # output gradient
    eY = self.one_hot(y)
    if loss_function == 'ce':
      d_al_theta = y_hat - eY.T
    elif loss_function == 'sq':
      d_al_theta = (y_hat - eY.T) * y_hat * (1 - y_hat)  

    self.d_weights = {}
    self.d_bias ={}
    self.d_h = {}
    self.d_a = {}
    no_of_samples = len(h[0])
    # print("no of sample")
    # print(no_of_samples)
    L = len(self.hidden_layers)
    self.d_a[L+1] = d_al_theta
    
    # hidden layers gradient
    for k in range(L, -1, -1):#2,1,0
     
      self.d_weights[k] = (1/no_of_samples) * self.d_a[k+1].dot(h[k].T)
      self.d_bias[k] = (1/no_of_samples) * np.sum(self.d_a[k+1], axis = 1, keepdims = True)
      self.d_h[k] = self.weights[k].T.dot(self.d_a[k+1])
      self.d_a[k] = self.d_h[k] * self.d_sigmoid(a[k])
    return self.d_weights, self.d_bias

    # def update_params(self, d_weights, d_bias, learning_rate):
      
  def get_prediction(self, y):
    # print(np.argmax(y, 1))
    return np.argmax(y, 0)
  
  def get_accuracy(self, prediction, y):
    return np.sum(prediction == y) / y.size

  def make_predictions(self, x):
    _, _, y_hatt = self.forward_prop(x)
    predictions = self.get_prediction(y_hatt)
    return predictions
    # return y_hatt, predictions

  def test_prediction(self, current_image, y):
    # current_image = X_train[:, index, None]
    prediction = self.make_predictions(current_image)
    label = y
    print("Prediction: ", prediction)
    print("Label: ", label)    
    current_image = current_image.reshape((28, 28)) * 255
    plt.gray()
    plt.imshow(current_image, interpolation='nearest')
    plt.show()

  def gradient_descent(self, input_neurons, learning_rate, epoch):
  
    for j in range(epoch):
      output_h, output_a, y_hat = ffnn.forward_prop(input_neurons)
      d_weights, d_bias = ffnn.backward_prop(output_h, output_a, y_hat, train_labels, 'ce')
      # updated_weight, updated_bias = update_params(d_weights, d_bias, learning_rate)
      #update parameters
      for i in d_weights:
        self.weights[i] = self.weights[i] - learning_rate * d_weights[i]
        self.bias[i] = self.bias[i] - learning_rate * d_bias[i]
    
      predictions = self.get_prediction(y_hat)
      accuracy = self.get_accuracy(predictions, train_labels)
      print("epoch______{} :   {}".format(j, accuracy))

    return self.weights, self.bias
    
if __name__ == "__main__":

  train_input_neurons = []
  test_input_neurons = []
  
  for i in range(len(train_images)):
    train_input_neurons.append(list(np.concatenate(train_images[i]).flat))

  for i in range(len(test_images)):
    test_input_neurons.append(list(np.concatenate(test_images[i]).flat))

  train_input_neurons = np.array(train_input_neurons).T
  test_input_neurons = np.array(test_input_neurons).T

  neurons_in_hidden_layers = [128, 64, 32]
  neurons_in_output_layer = 10
  ffnn = NN(len(train_input_neurons), neurons_in_hidden_layers, neurons_in_output_layer)
  
  # gradient descent or say training
  learning_rate = 0.01
  epoch = 300
  weight, bias = ffnn.gradient_descent(train_input_neurons, learning_rate, epoch)
  # output_h, output_a, y_hat = ffnn.forward_prop(train_input_neurons)
  print("validate training_____________________________________________________________________________________________________________")
  ffnn.test_prediction(train_input_neurons[:, 0:1], train_labels[0])
  ffnn.test_prediction(train_input_neurons[:, 1:2], train_labels[1])

  print("tedt training--------------------------------------------------------------------")
  ffnn.test_prediction(test_input_neurons[:, 0:1], test_labels[0])

  test_prediction = ffnn.make_predictions(test_input_neurons)
  test_accuracy = ffnn.get_accuracy(test_prediction, test_labels)
  print("test accuracy: {}".format(test_accuracy))

classes = {0 : "T-shirt/top", 1: "Trouser", 2: "Pullover", 3: "Dress", 4: "Coat",
          5: "Sandal", 6: "Shirt", 7: "Sneaker", 8: "Bag", 9: "Ankle Boot"}