# -*- coding: utf-8 -*-
"""Assign1_Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DZF9jRo63NRKMyRXlKpZ4ORfGh5uv6w7
"""

from keras.datasets import fashion_mnist
import numpy as np
import matplotlib.pyplot as plt

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()
train_images = np.array(train_images)
train_images = train_images / 255.0
test_images = np.array(test_images)
test_images = test_images / 255.0
print(train_images.shape)
print(test_images.shape)

class NN(object):
  def __init__(self, num_inputs, hidden_layers, num_outputs):
    self.num_inputs = num_inputs
    self.hidden_layers = hidden_layers
    self.num_outputs = num_outputs

    # nuerons in layers
    layers = [num_inputs] + hidden_layers + [num_outputs]

    # randomized weights and biases
    self.weights = []
    self.bias = []
    for i in range(len(layers)-1):
      self.weights.insert(i, (np.random.rand(layers[i+1], layers[i]) - 0.5))
      self.bias.insert(i, (np.random.rand(layers[i+1],1) - 0.5))
    
    
  def sigmoid(self, x):
    # print("----------------sigmoid _____________________")
    # print(x[0])
    x = x.T
    y = np.zeros(x.shape)
    # print("\n____________________________{}".format(y.shape))
    for i in range(y.shape[0]):
      # print(np.exp(-x[i]))
      y[i] = 1.0 / (1 + np.exp(-x[i]))
    # print("---------------------------------------------------first {}\n\n".format(y[0][0]))
    # print(y[0])
    # y = 1 / (1 + np.exp(-x))
    # print("--------------------------------------------result of sigmoid")
    # print(y.T)
    return y.T

  # def softmax(self, x):
  #   # print("in softmax")
  #   # print(x.shape)
  #   # print(x)
  #   x = x.T
  #   y = np.zeros(x.shape)
  #   # print(y.shape)
  #   for i in range(x.shape[0]):
  #     y[i] = np.exp(x[i])/sum(np.exp(x[i]))
  #   # print("\n{}".format(y.shape))
  #   # print(y[0])
  #   # A = np.exp(Z) / sum(np.exp(Z))
  #   return y.T

  def softmax_num_sable(self, x):
    # print("in stable softmax")
    # print(x.shape)
    # print(x)
    x = x.T
    y = np.zeros(x.shape)
    # print(y.shape)
    for i in range(x.shape[0]):
      exps = x[i] - np.max(x[i])
      exps = np.exp(exps)
      y[i] = exps/sum(exps)
    return y.T

  def forward_prop(self, X):
    hiden_operation = 1;
    self.ai = {}
    self.hi = {}
    self.hi[hiden_operation - 1] = X
    self.ai[hiden_operation - 1] = X

    for w,b in zip(self.weights, self.bias):
      if(hiden_operation < len(self.weights)):
        # print("-kkkkkkkkkkkkkkkkkk----------------------------------------------------------------------------------------------------b[0] {} b: \n{}\n\n".format(b[0], b.T))
        # print("-----------------------------------------------------------------w.h\n\n")
        self.ai[hiden_operation] = w.dot(self.hi[hiden_operation-1]) + b
        # print(w.dot(self.hi[hiden_operation-1]))
        # print("---------------------------------------------------------------a hidden\n\n")
        # print(w.shape)
        # print(self.hi[hiden_operation-1].shape)
        # print(self.ai[hiden_operation].shape)
        # print(self.ai[hiden_operation][0])
        self.hi[hiden_operation] = self.sigmoid(self.ai[hiden_operation])
        hiden_operation += 1
        
      else:
        self.ai[hiden_operation] = w.dot(self.hi[hiden_operation-1]) + b
        # print("_________next hidden")
        # print(self.ai[hiden_operation].shape)
        self.hi[hiden_operation] = self.softmax_num_sable(self.ai[hiden_operation])
    # print("y_hat---------------------------------------------------------------")
    # print(self.hi[hiden_operation])
    # print("\n\n{}".format(self.hi[hiden_operation]))
    return self.hi, self.ai, self.hi[hiden_operation]

        
  def get_prediction(self, y):
    # print(np.argmax(y, 1))
    return np.argmax(y, 0)
  
  def get_accuracy(self, prediction, y):
    return np.sum(prediction == y) / y.size

  def make_predictions(self, x):
    _, _, y_hatt = self.forward_prop(x)
    predictions = self.get_prediction(y_hatt)
    return predictions
    # return y_hatt, predictions

  def test_prediction(self, current_image, y):
    # current_image = X_train[:, index, None]
    prediction = self.make_predictions(current_image)
    label = y
    print("Prediction: ", prediction)
    print("Label: ", label)    
    current_image = current_image.reshape((28, 28)) * 255
    plt.gray()
    plt.imshow(current_image, interpolation='nearest')
    plt.show()

      
if __name__ == "__main__":

  train_input_neurons = []
  test_input_neurons = []
  
  for i in range(len(train_images)):
    train_input_neurons.append(list(np.concatenate(train_images[i]).flat))

  for i in range(len(test_images)):
    test_input_neurons.append(list(np.concatenate(test_images[i]).flat))

  train_input_neurons = np.array(train_input_neurons).T
  test_input_neurons = np.array(test_input_neurons).T

  neurons_in_hidden_layers = [128, 64, 32]
  neurons_in_output_layer = 10
  ffnn = NN(len(train_input_neurons), neurons_in_hidden_layers, neurons_in_output_layer)
  
  #for train images
  hi_train, ai_train, y_hat_train = ffnn.forward_prop(train_input_neurons)
  y_hat_train =  y_hat_train.T
  print("Probabiliy distribution for train images\nDistribution over first image\n{}".format(y_hat_train[0]))
  print("\nFor all images\n{}\n".format(y_hat_train.T))

  #for test images
  hi_test, ai_test, y_hat_test = ffnn.forward_prop(test_input_neurons)
  y_hat_test =  y_hat_test.T
  print("Probabiliy distribution for test test\nDistribution over first image\n{}\n".format(y_hat_test[0]))
  print("For all images\n{}".format(y_hat_test.T))